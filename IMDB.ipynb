{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "import lime\n",
    "\n",
    "# Download any necessary nltk files for nlp\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = './data/imdb_dataset.zip'\n",
    "extract_dir = './data/'\n",
    "data_dir = 'imdb_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the files \n",
    "zip_ref = zipfile.ZipFile(zip_file_path, 'r')\n",
    "zip_ref.extractall(extract_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing & Cleaning\n",
    "\n",
    "Let's begin by reading in all of our text files. We'll create their label according to their sentiment, either positive or negative. In addition we'll preprocess all the nexts by removing all non-alpha numeric characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:00<00:00, 19565.81it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 19556.72it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 19833.06it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 20140.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Regex to remove all Non-Alpha Numeric \n",
    "SPECIAL_CHARS = re.compile(r'([^a-z\\d!?.\\s])', re.IGNORECASE)\n",
    "\n",
    "def read_texts(glob_to_texts):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label = int(\"pos\" in glob_to_texts)\n",
    "    for text_name in tqdm(glob(glob_to_texts)):\n",
    "        with open(text_name, 'r') as text:\n",
    "            # Removing all non-alphanumeric\n",
    "            filter_text = SPECIAL_CHARS.sub('',  text.read())\n",
    "            texts.append(filter_text)\n",
    "            labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "# Get all training data\n",
    "train_pos_data = read_texts(os.path.join(data_dir, \"train/pos/*.txt\"))\n",
    "train_neg_data = read_texts(os.path.join(data_dir, \"train/neg/*.txt\"))\n",
    "\n",
    "# Get all test data\n",
    "test_pos_data = read_texts(os.path.join(data_dir, \"test/pos/*.txt\"))\n",
    "test_neg_data = read_texts(os.path.join(data_dir, \"test/neg/*.txt\"))\n",
    "\n",
    "train_texts = train_pos_data[0] + train_neg_data[0]\n",
    "train_labels = train_pos_data[1] + train_neg_data[1]\n",
    "\n",
    "test_texts = test_pos_data[0] + test_neg_data[0]\n",
    "test_labels = test_pos_data[1] + test_neg_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Texts\n",
    "\n",
    "Before we dive into actually building a model, let's take a look at our data. What does a positive example look like, and what does a negative example look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment example\n",
      "This is one of my favourite martial arts movies from Hong Kong. It is one of John Woos earliest films and one of only a few traditional martial arts movies he directed. You can see his influences from working under Chang Cheh in this film. The action is good the fight choreography is conducted by Fong Hak On who appears as one of the bad guys in the movie. It stars Wei Pei of Five Venoms fame and a whole host of faces familiar to fans of Golden Harvest and Shaw Brothers productions. The story line is interesting there are a few decent plot twists and the build up of the characters and their relationships with each other is cleverly done. This film has only had a VHS release in the UK. Media Asia have released a region 3 DVD and there are versions of it on DVD available from the USA. The film is lovely to watch in either its original language or in its English dubbed version. I highly recommend this movie.\n",
      "---------------------------\n",
      "Negative Sentiment example\n",
      "Luckily I did not pay to see this movie. Also I cannot even reveal any spoilers because I willingly WALKED OUT after forty minutes of the movie. It was that bad. I laughed once when the Yahoo! billboard fell on the guy and the theme song came on. However that was only because I thought it was making fun of it but then I realized it was yet ANOTHER product placement. br br I loved the cartoon. I used to watch it almost religiously. although i missed the last episode. I heard that they show Dr. Claw and it was nothing more than a Claw somebody comment on the shows page The cartoon had Penny and Brain alot more than the movie had as to that point. I hated the setup of the whole thing reminiscent of Robocop. Then Broderick screws with the whole feeling of Inspector Gadget. He is not nearly as clumsy as the cartoon was. Another fact is his gadgets actually work to the point I saw except for the oil slick. He also screwed with the tone of Wowsers which used to be in an excited tone. I felt so disappointed that they slaughtered the cartoon so badly. Everybody else felt that way too. Us 1417 year olds remember the cartoon fondly and we loved every minute of it.br br I went into the movie with an open mind knowing that they would have screwed with the cartoon. I was taken aback at how retarded the movie was. It relied on sight gags and stupid dialogue for humor. Disney relies on pain and physical humor to push a kids movie along. Product placement is pointless in this film and it shows. The wisecracking car is not that good at cracking wise. The gadgets look nice but they were almost overly glossy. The cartoon was a better look. The silly scenes were crap. In the 4045 minutes I watched the movie not one laugh was heard and they laughed at the DudleyDoRight preview. This movie should not be watched by people who want intelligence in their family entertainment. I highly recommend The Iron Giant which was sad but very very good. This movie is a travesty to the whole family drama.1\n"
     ]
    }
   ],
   "source": [
    "train_pos_sample_ind = np.random.randint(len(train_pos_data[0]))\n",
    "train_neg_sample_ind = np.random.randint(len(train_neg_data[0]))\n",
    "\n",
    "print(\"Positive Sentiment example\")\n",
    "print(train_pos_data[0][train_pos_sample_ind])\n",
    "print(\"---------------------------\")\n",
    "print(\"Negative Sentiment example\")\n",
    "print(train_neg_data[0][train_neg_sample_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets. We'll create a validation test set with 10% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1,\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also figure out the word frequencies from our dataset! The below code will count how frequent each word is in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:00<00:00, 821.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_counts = defaultdict(int)\n",
    "\n",
    "# Compute the frequency of each unique\n",
    "for text in tqdm(train_texts + val_texts + test_texts):\n",
    "    # Splits sentences \n",
    "    for word in word_tokenize(text):\n",
    "        word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Most frequent words\n",
      "the : 572555\n",
      ". : 464055\n",
      "a : 308569\n",
      "and : 307390\n",
      "of : 284990\n",
      "to : 263469\n",
      "is : 208232\n",
      "in : 172889\n",
      "I : 143088\n",
      "that : 132406\n",
      "it : 129402\n",
      "br : 120118\n",
      "this : 120033\n",
      "was : 94612\n",
      "The : 88649\n",
      "as : 84731\n",
      "with : 84522\n",
      "movie : 82646\n",
      "for : 82464\n",
      "film : 73666\n",
      "but : 67998\n",
      "on : 64518\n",
      "are : 57626\n",
      "not : 56836\n",
      "have : 54601\n",
      "his : 54509\n",
      "you : 53835\n",
      "be : 52321\n",
      "! : 49164\n",
      "one : 46411\n",
      "at : 43261\n",
      "by : 42797\n",
      "he : 42732\n",
      "an : 41316\n",
      "all : 41050\n",
      "who : 38857\n",
      "from : 38596\n",
      "like : 37546\n",
      "its : 35987\n",
      "they : 35587\n",
      "so : 34132\n",
      "or : 33799\n",
      "about : 33497\n",
      "her : 32916\n",
      "just : 32726\n",
      "has : 32698\n",
      "? : 32338\n",
      "out : 32303\n",
      "This : 29190\n",
      "some : 28447\n",
      "good : 27463\n",
      "more : 27144\n",
      "very : 26330\n",
      "... : 24888\n",
      "what : 24275\n",
      "up : 23951\n",
      "would : 23759\n",
      "It : 23746\n",
      "can : 23382\n",
      "when : 23090\n",
      "time : 22826\n",
      "if : 22763\n",
      "which : 22509\n",
      "really : 22236\n",
      "only : 22055\n",
      "their : 22030\n",
      "see : 21812\n",
      "were : 21772\n",
      "had : 21681\n",
      "even : 21542\n",
      "story : 21449\n",
      "there : 20943\n",
      "no : 20929\n",
      "my : 20586\n",
      "me : 20269\n",
      "she : 19301\n",
      "than : 19107\n",
      "much : 18412\n",
      "been : 18202\n",
      "get : 17783\n",
      "into : 17718\n",
      "will : 17273\n",
      "other : 17125\n",
      "him : 16903\n",
      "bad : 16793\n",
      "because : 16748\n",
      "people : 16720\n",
      "do : 16595\n",
      "great : 16545\n",
      "well : 15933\n",
      "most : 15849\n",
      "we : 15494\n",
      "them : 15344\n",
      "first : 15330\n",
      "made : 15250\n",
      "also : 15195\n",
      "movies : 15123\n",
      "make : 15102\n",
      "how : 15085\n",
      "films : 15075\n"
     ]
    }
   ],
   "source": [
    "print(\"100 Most frequent words\")\n",
    "for word in sorted(word_counts, reverse=True, key=lambda w: word_counts[w])[:100]:\n",
    "    print(word, \":\", word_counts[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "In order to extract information from text, we'll vectorize our word sequences. In other words, we'll transform our sentences into numerical features. There are many vectorization or embedding techniques such as Bag of Words, Pre-Trained word embeddings, but in our case we'll be using **TF-IDF**.\n",
    "\n",
    "TF-IDF stands for \"Term Frequency, Inverse Document Frequency\". It's a technique that converts words into an importance score of each word in the document based on how they appear accros multiple documents. Intuitively, the TF-IDF score of a word is high when it is frequently found in a document. However, if the word appears in many documents, this word is not a unique identifier, and as such, will have a lower score. For example, common words such as \"the\" and \"and\" will have low score since they appear in many documents. \n",
    "\n",
    "Our TF-IDF vectorizer is a little more sophisticated than described above. It not only looks at the TF-IDF scores of individiual words, but also of **bigrams**, pairs of words that occur next to each other in our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 2), tokenizer=word_tokenize,\n",
    "                      min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "                      smooth_idf=1, sublinear_tf=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit our vectorizer to our entire corpus of words, which includes the training, validation, and test sets. Once fitted, we'll transform each subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Vectorizer TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=1,\n",
      "        stop_words=None, strip_accents='unicode', sublinear_tf=1,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function word_tokenize at 0x7f819002b510>, use_idf=1,\n",
      "        vocabulary=None)\n",
      "Fitting to all docs...\n",
      "Transforming train docs...\n",
      "Transforming val docs...\n",
      "Transforming test docs...\n"
     ]
    }
   ],
   "source": [
    "print(\"Created Vectorizer %s\" % vec)\n",
    "print(\"Fitting to all docs...\")\n",
    "vec.fit(train_texts + val_texts + test_texts)\n",
    "print(\"Transforming train docs...\")\n",
    "trn_term_doc = vec.transform(train_texts)\n",
    "print(\"Transforming val docs...\")\n",
    "val_term_doc = vec.transform(val_texts)\n",
    "print(\"Transforming test docs...\")\n",
    "test_term_doc = vec.transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Training our model\n",
    "* First, we use a special kind of SVM model called a Naive-Bayes SVM model. It is based on the [sklearn.svm](http://scikit-learn.org/stable/modules/svm.html#svc) module to create a [vector classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "* Next, we pass our training texts and labels to the classifier's fit method, which trains our model.\n",
    "* Finally, the test texts and labels are passed to the score method to see how well we trained our model. This will return a float between 0-1 indicating our accuracy on the test data set\n",
    "\n",
    "### Aside on SVM's (if you don't know what they are)\n",
    "We have 2 colors of balls on the table that we want to separate.\n",
    "![](http://i.imgur.com/zDBbD.png)\n",
    "\n",
    "We get a stick and put it on the table, this works pretty well right?\n",
    "\n",
    "![](http://i.imgur.com/aLZlG.png)\n",
    "\n",
    "Some villain comes and places more balls on the table, it kind of works but one of the balls is on the wrong side and there is probably a better place to put the stick now. What a troll. \n",
    "\n",
    "![](http://i.imgur.com/kxWgh.png)\n",
    "\n",
    "SVMs try to put the stick in the best possible place by having as big a gap on either side of the stick as possible. \n",
    "\n",
    "![](http://i.imgur.com/ePy4V.png)\n",
    "\n",
    "Now when the villain returns, the stick is still in a pretty good spot.\n",
    "\n",
    "![](http://i.imgur.com/BWYYZ.png)\n",
    "\n",
    "There is another trick in the SVM toolbox that is even more important. Say the villain has seen how good you are with a stick so he gives you a new challenge.\n",
    "\n",
    "![](http://i.imgur.com/R9967.png)\n",
    "\n",
    "There’s no stick in the world that will let you split those balls well, so what do you do? You flip the table of course! Throwing the balls into the air. Then, with your pro ninja skills, you grab a sheet of paper and slip it between the balls.\n",
    "\n",
    "![](http://i.imgur.com/WuxyO.png)\n",
    "\n",
    "Now, looking at the balls from where the villain is standing, they balls will look split by some curvy line.\n",
    "\n",
    "![](http://i.imgur.com/gWdPX.png)\n",
    "\n",
    "Boring people the call balls data, the stick a classifier, the biggest gap trick optimization, call flipping the table kernelling and the piece of paper a hyperplane.\n",
    "\n",
    "### But what is a Naive-Bayes SVM?\n",
    "\n",
    "This SVM uses a simple, but clever kernelling trick. It transforms the input by multiplying it with a matrix that holds the log-probabilities of each feature (unigram, bigram, or trigram) being from a positive or negative example. This simple trick can boost the performance of the SVM to be on par with most state-of-the-art deep learning techniques (but this SVM can run on our measly computer)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual='auto', verbose=0):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.verbose = verbose\n",
    "        self._clf = None\n",
    "        print(\"Creating model with C=%s\" % C)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.score(x.multiply(self._r), y)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y == y_i].sum(0)\n",
    "            return (p + 1) / ((y == y_i).sum() + 1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x, 1, y) / pr(x, 0, y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        if self.dual == 'auto':\n",
    "            self.dual = x_nb.shape[0] <= x_nb.shape[1]\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, verbose=self.verbose)\n",
    "        self._clf.fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal parameters\n",
    "We'll perform a grid search across the C parameter to find the optimal parameter for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with C=0.01\n",
      "Creating model with C=0.01\n",
      "Model had val score of 0.8532\n",
      "New maximum score improved from -inf to 0.8532\n",
      "Fitting with C=0.1\n",
      "Creating model with C=0.1\n",
      "Model had val score of 0.8596\n",
      "New maximum score improved from 0.8532 to 0.8596\n",
      "Fitting with C=1.0\n",
      "Creating model with C=1.0\n",
      "Model had val score of 0.8936\n",
      "New maximum score improved from 0.8596 to 0.8936\n",
      "Fitting with C=10.0\n",
      "Creating model with C=10.0\n",
      "Model had val score of 0.9148\n",
      "New maximum score improved from 0.8936 to 0.9148\n",
      "Fitting with C=100.0\n",
      "Creating model with C=100.0\n",
      "Model had val score of 0.9164\n",
      "New maximum score improved from 0.9148 to 0.9164\n",
      "Best score with C=100.0 is 0.9164\n"
     ]
    }
   ],
   "source": [
    "# Search for the appropriate C\n",
    "Cs = [1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "\n",
    "best_model = None\n",
    "best_val = -float(\"inf\")\n",
    "best_C = None\n",
    "for C in Cs:\n",
    "    print(\"Fitting with C={}\".format(C))\n",
    "    model = NbSvmClassifier(C=C, verbose=0).fit(trn_term_doc, train_labels)\n",
    "    # Evaluate the model\n",
    "    val_preds = model.predict(val_term_doc)\n",
    "    score = np.mean(val_labels == val_preds)\n",
    "\n",
    "    print(\"Model had val score of %s\" % score)\n",
    "    if score > best_val:\n",
    "        print(\"New maximum score improved from {} to {}\".format(best_val, score))\n",
    "        best_model = model\n",
    "        best_val = score\n",
    "        best_C = C\n",
    "score = best_val\n",
    "print(\"Best score with C={} is {}\".format(best_C, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91308"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score(test_term_doc, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "From this tutorial, we learned how to work with text data and use a basic embedding. In addition, we realize that deep learning isn't always the way to go! We trained a fast and powerful linear-ish model that achieved ~**91**%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LIME to Explain our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vec, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10215365, 0.89784635]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.predict_proba([test_texts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhmul/.conda/envs/dl/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id: 83\n",
      "Probability(positive) = 0.9973952074379704\n",
      "True class: positive\n"
     ]
    }
   ],
   "source": [
    "idx = 83\n",
    "exp = explainer.explain_instance(test_texts[idx], c.predict_proba, num_features=6)\n",
    "print('Document id: %d' % idx)\n",
    "print('Probability(positive) =', c.predict_proba([test_texts[idx]])[0,1])\n",
    "print('True class: %s' % class_names[test_labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('810', 0.13494734271430656),\n",
       " ('favorite', 0.07177037644747612),\n",
       " ('hilarious', 0.041643969385283974),\n",
       " ('beautiful', 0.03616023364942977),\n",
       " ('easy', 0.031017250463696827),\n",
       " ('hoping', -0.027000732521257453)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig = exp.as_pyplot_figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
