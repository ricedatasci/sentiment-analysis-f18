{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/aclImdb/\"\n",
    "RANDOM_SEED = 7575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_text_dir(datadir):\n",
    "    path = Path(datadir)\n",
    "    for fil in path.glob(\"*.txt\"):\n",
    "        yield fil\n",
    "        \n",
    "# Regex to remove all Non-Alpha Numeric \n",
    "SPECIAL_CHARS = re.compile(r'([^a-z\\d!?.\\s])', re.IGNORECASE)\n",
    "\n",
    "def read_files(files):\n",
    "    for fil in files:\n",
    "        with open(fil, \"r\") as f:\n",
    "            yield SPECIAL_CHARS.sub(\"\", f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def read_data(_dir):\n",
    "    splits = [\"train\", \"test\"]\n",
    "    labels = {\"pos\": 1, \"neg\": 0}\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for split, label in product(splits, labels.keys()):\n",
    "        datadir = os.path.join(_dir, f\"{split}/{label}\")\n",
    "        text_gen = read_files(read_text_dir(datadir))\n",
    "        dfs.append(\n",
    "            pd.DataFrame({\"text\": list(tqdm(text_gen)), \"split\": split, \"label\": labels[label]})\n",
    "        )\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7437511def0646769a097d0286d0d564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869c11e1424b4da89ceb0e9695f5626f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac9fa287d0c4768b9212014a66bc2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f3b1b501a9467b9a19b440af26aa0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_df = read_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fear of a Black Hat is a superbly crafted film...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Many reviews Ive read reveals that most people...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A nicely done thriller with plenty of sex in i...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Im going to keep this fairly brief as to not s...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At first I thought the Ring would be a more th...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  split  label\n",
       "0  Fear of a Black Hat is a superbly crafted film...  train      1\n",
       "1  Many reviews Ive read reveals that most people...  train      1\n",
       "2  A nicely done thriller with plenty of sex in i...  train      1\n",
       "3  Im going to keep this fairly brief as to not s...  train      1\n",
       "4  At first I thought the Ring would be a more th...  train      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = imdb_df[imdb_df.split == \"train\"]\n",
    "test_df = imdb_df[imdb_df.split == \"test\"]\n",
    "\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing our data\n",
    "\n",
    "In order to extract information from text, \n",
    "we need to vectorize our word sequences. \n",
    "In other words, we'll transform our sentences into numerical features. \n",
    "There are many vectorization or embedding techniques such as Bag of Words, \n",
    "Pre-Trained word embeddings, but in our case we'll be using a representation known as [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "TF-IDF stands for \"Term Frequency, Inverse Document Frequency\", \n",
    "and is the product of two indendent statistics: Term Frequency (word counts) \n",
    "and Inverse Document Frequency (1 / number of documents containing a word).\n",
    "There are [various modifications often made](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition), but the simplest\n",
    "formulation simply uses term frequency:\n",
    "$$\n",
    "tf(i, d) = \\text{raw count} = f_{i, d}\n",
    "$$\n",
    "\n",
    "for a word $i$ and document $d$  \n",
    "$$\n",
    "idf(i, D) = \\log{\\frac{N}{1 + |\\{ d \\in D : i \\in d \\}|}}\n",
    "$$\n",
    "\n",
    "for a collection of documents $D$, so that our TFIDF value for a word $i$ and document $d$ is:\n",
    "$$\n",
    "TFIDF(i, d) = tf(i, d) * idf(i, D)\n",
    "$$\n",
    "\n",
    "Note that a 1 is included in the denominator of the definition of `idf` to prevent\n",
    "from division by zero.  Of course, if our vocabulary is generated from our\n",
    "corpus (collection of documents) then we won't run into this problem.\n",
    "\n",
    "### intuition\n",
    "\n",
    "The TF-IDF score of a word is high when it is frequently found in a document. \n",
    "However, if the word appears in many documents, i.e. is not a good discriminator, \n",
    "it will have a lower score. \n",
    "For example, common words such as \"the\" or \"and\" will have low score since they appear in many documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation\n",
    "\n",
    "We'll use the scikit-learn implementation \n",
    "[`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "for the most part, but you're encouraged to implement your own in\n",
    "`feature.py`.  There is a class stubbed out that is designed compliant\n",
    "to the sklearn transformer api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_corpus = train_df.text\n",
    "test_corpus = test_df.text\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2), # sizes of ngrams to use\n",
    "    lowercase=False, # convert text to lowercase first\n",
    "    min_df=0.001,\n",
    "    max_df=0.95,\n",
    "    max_features=5000\n",
    ")\n",
    "train_feat = vectorizer.fit_transform(train_corpus)\n",
    "test_feat = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "train: (25000, 5000)\n",
      "test: (25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes:\\ntrain: {train_feat.shape}\\ntest: {test_feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 25000 examples with 10000 features each as desired.\n",
    "Note that we fit the vectorizer only on the training corpus, but\n",
    "transform all features.  This is very important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling our data\n",
    "\n",
    "Now we have a simple task: binary classification given numerical feature data.\n",
    "We can now use many common models to predict the sentiment (positive or negative)\n",
    "of this data!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_feat, imdb_df[imdb_df.split == \"train\"].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "accuracy: 0.88712\n",
      "f1: 0.887980311209908\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "y_pred = lr_clf.predict(test_feat)\n",
    "y_true = imdb_df[imdb_df.split == \"test\"].label\n",
    "print(f\"accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "print(f\"f1: {f1_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall we do well! Hyper-parameter tuning will probably push our accuracy and f1\n",
    "above .9 which is quite good.  Keep in mind this is just considering 10,000 words\n",
    "and only their counts, not even placement in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualizing / understanding\n",
    "\n",
    "Recent work has been done to attempt to explain why models emit \n",
    "a particular classification given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_fn(clf):\n",
    "    \"\"\"\n",
    "    Returns a function that takes as parameter a text instance and \n",
    "    returns the output of clf.predict_proba of the given TFIDF repr.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _predict_proba(_input):\n",
    "        if type(_input) is str:\n",
    "            feat = vectorizer.transform([_input])\n",
    "        else:\n",
    "            feat = vectorizer.transform(_input)\n",
    "    \n",
    "        return clf.predict_proba(feat)\n",
    "    \n",
    "    return _predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=[\"negative\", \"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c78247e474b42f9a99249adde714eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value=\"The Room was one of the funniest movies I've ever seen!\", description='text_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(text_instance=\"The Room was one of the funniest movies I've ever seen!\")\n",
    "def lime_example(text_instance):\n",
    "    exp = explainer.explain_instance(text_instance, classifier_fn=clf_fn(lr_clf))\n",
    "    exp.show_in_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
